{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKS6r2mwxfuiZBmfXsrgoO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dtoralg/TheValley-Intro_unsupervised/blob/main/%5B01%5D%20-%20Intro_No_Supervisados/%5B01%5D%20-%20Notebooks/E3_Principal_Component_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z14_gFVCCp56"
      },
      "source": [
        "# Ejercicio de Análisis de Componentes Principales (PCA): Análisis del Dataset IRIS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducción"
      ],
      "metadata": {
        "id": "r0SWMZMqDf3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "En este tercer ejercicio vamos a realizar un Análisis de Componentes Principales (PCA), donde aplicaremos técnicas de reducción dimensional para analizar y visualizar el famoso dataset IRIS. PCA es una técnica muy potente que nos permite transformar un conjunto de variables posiblemente correlacionadas en un conjunto más pequeño de variables no correlacionadas, llamadas componentes principales.\n",
        "\n",
        "Esta técnica es lineal, por lo que solamente capturará relaciones lineales entre variables. En PCA buscamos encontrar las componentes principales (ejes) que consiguen que la covarianza entre dos variables tienda a cero.\n",
        "\n",
        "Estas covarianzas se pueden ordenar de mayor a menor, con lo cual podremos encontrar aquellos ejes que explican la mayor parte de la varianza del dataset, pudiendo prescindir de aquellos ejes que representan un porcentaje menor de la misma - aplicando así una reducción dimensional.\n",
        "\n",
        "### Objetivos del Ejercicio\n",
        "\n",
        "- **Comprender los conceptos básicos del PCA** y su utilidad en el análisis de datos.\n",
        "- **Preprocesar los datos** del dataset IRIS para prepararlos adecuadamente para el análisis.\n",
        "- **Aplicar PCA** para reducir la dimensionalidad de los datos del IRIS.\n",
        "- **Visualizar los resultados de PCA** en 2D para interpretar las relaciones entre las muestras.\n",
        "\n",
        "### Descripción del Dataset IRIS\n",
        "\n",
        "El dataset IRIS es uno de los conjuntos de datos más conocidos y utilizados en el campo del Machine Learning y la estadística. Contiene 150 muestras de flores, cada una descrita por cuatro características métricas:\n",
        "\n",
        "- **Sepal Length (cm):** Longitud del sépalo\n",
        "- **Sepal Width (cm):** Ancho del sépalo\n",
        "- **Petal Length (cm):** Longitud del pétalo\n",
        "- **Petal Width (cm):** Ancho del pétalo\n",
        "\n",
        "Además, cada muestra está etiquetada con una de las tres especies de iris:\n",
        "\n",
        "- **Iris-setosa**\n",
        "- **Iris-versicolor**\n",
        "- **Iris-virginica**\n",
        "\n",
        "En este ejercicio, utilizaremos las características métricas para realizar PCA y analizaremos cómo las nuevas componentes principales nos ayudan a entender y visualizar mejor los datos.\n",
        "\n",
        "### Estructura del Notebook\n",
        "\n",
        "1. **Importación de Librerías y Carga de Datos:** Preparar el entorno de trabajo e importar el dataset IRIS.\n",
        "2. **Exploración de Datos:** Analizar las características principales del dataset IRIS.\n",
        "3. **Preprocesamiento de Datos:** Normalizar los datos para asegurar que todas las características contribuyan equitativamente al análisis.\n",
        "4. **Aplicación de PCA:** Implementar PCA para reducir la dimensionalidad de los datos.\n",
        "5. **Visualización de Resultados:** Graficar las componentes principales en 2D para interpretar los clusters formados.\n",
        "\n",
        "### Requisitos\n",
        "\n",
        "Para seguir este notebook, asegúrate de tener instaladas las siguientes librerías de Python:\n",
        "\n",
        "- `numpy`\n",
        "- `pandas`\n",
        "- `matplotlib`\n",
        "- `seaborn`\n",
        "- `scikit-learn`\n",
        "\n",
        "Puedes instalarlas utilizando el siguiente comando:\n",
        "\n",
        "```bash\n",
        "pip install numpy pandas matplotlib seaborn scikit-learn\n"
      ],
      "metadata": {
        "id": "B2DD2FjrVob4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio de PCA"
      ],
      "metadata": {
        "id": "iXH-UmUEDZ6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos los datos del dataset iris para seguir con nuestro análisis."
      ],
      "metadata": {
        "id": "z08LDD6SLG-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "2hBTFRZ3LZ2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = px.data.iris() # Otra forma de acceder a IRIS, con plotly express\n",
        "features = [\"sepal_width\", \"sepal_length\", \"petal_width\", \"petal_length\"] # Marcamos las variables numéricas\n",
        "\n",
        "fig = px.scatter_matrix(\n",
        "    df,\n",
        "    dimensions=features,\n",
        "    color=\"species\"\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "zq5BUuwbJ7XF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extraemos el modelo PCA de los datos de IRIS y hallamos las componentes principales para almacenarlas en `components`"
      ],
      "metadata": {
        "id": "suhsDm0eNIFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_norm = StandardScaler().fit_transform(df[features].values)\n",
        "pca = PCA()\n",
        "components = pca.fit_transform(df_norm)"
      ],
      "metadata": {
        "id": "9r6P7ogIMqLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con la siguiente función graficamos los distintos datapoints respecto a las combinaciones lineales de los 4 ejes principales que hemos obtenido.\n",
        "\n",
        "Recordemos que como en el dataset IRIS tenemos 4 variables, el número máximo de Componentes Principales que podemos obtener siempre será 4."
      ],
      "metadata": {
        "id": "HaLh1sZDN4DD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = {\n",
        "    str(i): f\"PC {i+1} ({var:.1f}%)\"\n",
        "    for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n",
        "}\n",
        "\n",
        "fig = px.scatter_matrix(\n",
        "    components,\n",
        "    labels=labels,\n",
        "    dimensions=range(4),\n",
        "    color=df[\"species\"]\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "oToCktYlKKAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora debemos evaluar cual de nuestras 4 componentes principales explica la mayor parte de la varianza. Para ello podemos utilizar la función `explained_variance_ratio_`."
      ],
      "metadata": {
        "id": "iyKc2qE9Ocla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exp_var_cumul = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "fig = px.area(\n",
        "              x=range(1, exp_var_cumul.shape[0] + 1),\n",
        "              y=exp_var_cumul\n",
        ")\n",
        "\n",
        "fig.update_layout(xaxis_type='category')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "uewiNF2gOccK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se aprecia que con 1 eje podemos explicar aprox. 70% de la varianza y con 2 ejes alcanzamos prácticamente el 90% de la misma.\n",
        "\n",
        "Con el método PCA podemos fijar `n_components` para definir el número de ejes principales con los que nos quedamos. En este caso tomamos n = 2 para llegar al 90% de la varianza como hemos visto en el gráfico anterior."
      ],
      "metadata": {
        "id": "3_4-gN28OOcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)\n",
        "components = pca.fit_transform(df_norm)\n",
        "\n",
        "fig = px.scatter(components,\n",
        "                 x=0,\n",
        "                 y=1,\n",
        "                 color=df['species'])\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "SYR5ErTUKT0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente podemos combinar todo de manera gráfica en un solo scatterplot: tanto los datapoints como las componentes principales que hemos seleccionado. Para ello dibujaremos los autovectores con un autovalor unitario definido por la raiz del mismo.\n",
        "\n",
        "$loading = autovector * \\sqrt autovalor$"
      ],
      "metadata": {
        "id": "_nGGrVmBTEsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "loadings = pca.components_.T * np.sqrt(pca.explained_variance_) # Calculamos los loadings\n",
        "\n",
        "fig = px.scatter(components, x=0, y=1, color=df['species']) # Graficamos los datapoints\n",
        "\n",
        "for i, feature in enumerate(features):\n",
        "    fig.add_annotation(\n",
        "        ax=0, ay=0,\n",
        "        axref=\"x\", ayref=\"y\",\n",
        "        x=loadings[i, 0],\n",
        "        y=loadings[i, 1],\n",
        "        showarrow=True,\n",
        "        arrowsize=2,\n",
        "        arrowhead=2,\n",
        "        xanchor=\"right\",\n",
        "        yanchor=\"top\"\n",
        "    )\n",
        "    fig.add_annotation(\n",
        "        x=loadings[i, 0],\n",
        "        y=loadings[i, 1],\n",
        "        ax=0, ay=0,\n",
        "        xanchor=\"center\",\n",
        "        yanchor=\"bottom\",\n",
        "        text=feature,\n",
        "        yshift=5,\n",
        "    )\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "oVHEQpG8TFjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extra: Visualización de 3 Componentes Principales en 3D"
      ],
      "metadata": {
        "id": "07YrM18mSt7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=3)\n",
        "components = pca.fit_transform(df_norm)\n",
        "\n",
        "total_var = pca.explained_variance_ratio_.sum() * 100\n",
        "\n",
        "fig = px.scatter_3d(\n",
        "    components, x=0, y=1, z=2, color=df['species'],\n",
        "    title=f'Total Explained Variance: {total_var:.2f}%',\n",
        "    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'}\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "QPVuz82WKWPF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}